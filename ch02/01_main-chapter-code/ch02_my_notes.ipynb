{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/three-main-stages-of-coding-an-llm-stage1-step1.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Understanding word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "<b>Why do we need embeddings?</b>\n",
    "- <span style=\"color:red\">Deep neural network (NN) models, including LLMs cannot process text data directly. Since, the text data is categorical, it's not compatible with mathematical operations used to train NNs.</span>\n",
    "- So, we need a way <span style=\"color:#4ea9fb\">to represent non numeric data (words/text) in a continuous numbers, a format that NNs can understand and process</span>.\n",
    "\n",
    "<b>What's an embedding?</b>\n",
    "- The concept of <span style=\"color:#4ea9fb\"><b>converting text (or other data) into numerical vector representations.</b></span>\n",
    "- In other words, embedding is a mapping from discrete objects (words, image, or entire documents) into a point in continuous high dimensional space. \n",
    "\n",
    "<b>Different type of embeddings</b>\n",
    "- While *word embeddings* are the most common form of text embedding, there are other type of embeddings such as subword/token, sentence, paragraph, document, etc.\n",
    "  - Since GPT-like LLMs learn to generate one word at a time, we will focus on **word embeddings**.\n",
    "- Refer [https://prasanth.io/Knowledge/Tech/Embeddings](https://prasanth.io/Knowledge/Tech/Embeddings) for different type of embeddings.\n",
    "- For *retrieval-augmentated generation*, sentence or paragraph embeddings are more popular choices.\n",
    "\n",
    "<b>How to embed different data types?</b>\n",
    "- Using a specific NN layer or another pretrained NN model, we can embed different data types - such as text, image, video, etc. \n",
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">💡Different data types require different embedding models. <span style=\"color:red\">Embedding model designed for text data would not be suitable for embedding audio or video data.</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/different-embedding-models-for-different-data-types.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Word2Vec</i> - Most popular word embedding</b>\n",
    "- <span style=\"color:#4ea9fb\">The main idea behind Word2Vec is that <b>words that appear in similar contexts tend to have similar meanings</b></span>. Consequently, when projected into two-dimensional word embeddings for visualization purposes, similar terms are clustered together.\n",
    "- For more details, refer [https://prasanth.io/Knowledge/Tech/Word2Vec](https://prasanth.io/Knowledge/Tech/Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/word-embeddings-projected-in-two-dimension-example.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Why don't we use <i>Word2Vec</i> for LLMs?</b>\n",
    "- <span style=\"color:#4ea9fb\">LLMs commonly produce their own embeddings as part of the input layer, and are updated during training</span>.\n",
    "- <span style=\"color:green\">The advantage of optimizing the embeddings as part of the LLM training is that the embeddings are optimized to the specific data and task at hand</span>.\n",
    "  - LLMs can also create contextualized output embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What's an optimal Embedding Size</b>?\n",
    "- It's <span style=\"color:#4ea9fb\">a trade off between performance and effficiency</span>.\n",
    "- For more details on embedding size of various GPTs, refer https://prasanth.io/Knowledge/Tech/GPT-comparison.\n",
    "  - For e.g., GPT-1 and GPT-2 Small (both 117M parameters) use an embedding size of 768 dimensions, where as GPT-3 Davinci (175B parameters) use an embedding size of 12,288 dimensions (16x of the former)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tokenizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we tokenize text, which means breaking text into smaller units, such as individual words and punctuation characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What's tokenizing?</b>\n",
    "- Split input text into individual tokens (or words or sub-words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">💡The image shown here is slightly an oversimplied version. <br>&nbsp;&nbsp;&nbsp;- <span style=\"color:red\">Between <b>Token IDs</b> and <b>Token embeddings</b>, there's an intermediate sliding window based process.<br>&nbsp;&nbsp;&nbsp;- The <b>token embedding</b> will be added with <b>positional embeddings</b> to create the final <b>input embeddings</b> for the decoder.</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/tokenizing-text-block-diagram.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load raw text we want to work with\n",
    "- [The Verdict by Edith Wharton](https://en.wikisource.org/wiki/The_Verdict) is a public domain short story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'the-verdict.txt' already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "if not os.path.exists(file_path):\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "        \"the-verdict.txt\"\n",
    "    )\n",
    "    print(f\"Downloading file from: '{url}' to '{file_path}'...\")\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "else:\n",
    "    print(f\"File '{file_path}' already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in text: 20479\n",
      "First 100 characters in text: \n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, \"r\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(f\"Total characters in text: {len(raw_text)}\")\n",
    "print(f\"First 100 characters in text: \\n{raw_text[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **goal is to tokenize and embed this text for an LLM**\n",
    "- Let's develop a simple tokenizer based on some simple sample text that we can then later apply to the text above\n",
    "- The following regular expression will split on whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'word.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, word. This, is a test.\"\n",
    "# Split on whitespace character\n",
    "result = re.split(r\"(\\s)\", text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We don't only want to split on whitespaces but also commas and periods, so let's modify the regular expression to do that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'word', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "# Split on whitespace, commans, and period character\n",
    "result = re.split(r\"([.,]|\\s)\", text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Should we remove whitspaces or not during tokenization?</b>\n",
    "- <span style=\"color:green\"><b>Removing whitespaces reduces the memory and computing requirements</b></span>\n",
    "- <span style=\"color:red\">Keeping whitespaces can be useful, if we train models that are sensitive to the exact structure of the text (e.g., Python code, which is sensitive to indentation and spacing).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, this creates empty strings, let's remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'word', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# Strip whitespace from each tiem and then filter out any empty strings\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This looks pretty good, but let's also handle other types of punctuation, such as periods, question marks, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip() != \"\"]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/simple-tokenization-example-of-sample-text.png\" width=\"450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is pretty good, and we are now ready to apply this tokenization to the raw text loaded from `the-verdict.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of tokens: 4690\n",
      "First 30 tokens: \n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip() != \"\"]\n",
    "print(f\"No. of tokens: {len(preprocessed)}\")\n",
    "print(f\"First 30 tokens: \\n{preprocessed[:30]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Converting tokens into token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we convert the text tokens into token IDs using vocabulary that we can process via embedding layers later to generate token embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/vocabulary-to-convert-text-tokens-to-token-ids-example.png\" width=\"650px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From these tokens, we can now build a vocabulary that consists of all the unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))  # Sort individual tokens in alphabetical order\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "vocab = {token: id for id, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below are the first 20 entries in this vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    if i < 20:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below, we illustrate the tokenization of a short sample text using a small vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/tokenization-example-using-a-sample-small-vocabulary.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Putting it now all together into a **simple text tokenizer** class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        # Store the vocabulary for access in encode and decode methods\n",
    "        self.str_to_int = vocab\n",
    "        # Create an inverse vocabulary that maps token ids to the original text tokens\n",
    "        self.int_to_str = {id: token for token, id in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess the input text into token ids\n",
    "        \"\"\"\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        Convert token ids back to the original text\n",
    "        \"\"\"\n",
    "        text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "\n",
    "        # Remove spaces before the specified punctuation marks\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r\"\\1\", text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `encode` function turns text into token IDs\n",
    "- The `decode` function turns token IDs back into text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/tokenizer-encoder-and-decoder-implementations-example.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"It\\'s the last he painted, you know,\" \\n           Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the vocab created from `the-verdict.txt`\n",
    "tokenizer = SimpleTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can decode the integers back into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Problem with tokenizing words that are not in the vocabulary</b>\n",
    "- <span style=\"color:red\">The word \"Hello\" was not usd in the `the_verdict.txt` text, so it's not in the vocabulary.</span> So, the code cell below will fail.\n",
    "- <p style=\"color:black; background-color:#F5C780; padding:15px\">💡 So, a large and diverse training sets are needed to extend the vocabulary when working with LLMs.</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if 'Hello' is in the vocabulary: False\n",
      "KeyError: 'Hello'\n"
     ]
    }
   ],
   "source": [
    "flag = \"Hello\" in tokenizer.str_to_int.keys()\n",
    "print(f\"Checking if 'Hello' is in the vocabulary: {flag}\")\n",
    "\n",
    "text = \"Hello, do you like tea?\"\n",
    "try:\n",
    "    tokenizer.encode(text)\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Adding special context tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Why do we need special tokens?</b>\n",
    "- <span style=\"color:#4ea9fb\"><b>Special tokens help LLM with additional context</b></span> like unknown words and document boundaries.\n",
    "-  Some of these special tokens are\n",
    "   - `[UNK]` or `<|unk|>` (unknown): Represents unknown words (words that are not in the vocabulary)\n",
    "   - `[EOS]` (end of sequence) or `<|endoftext|>`: Positioned at the end of the text. Acts as a marker for LLM, signalling the end of a particular segment, such as text or document (usually used to concatenate multiple unrelated text, e.g., two different Wikpedia articles or two different books, and so on). \n",
    "   - `[BOS]` (beginning of sequence): Positioned at the beginning of the text. Acts as a marker, signalling the beginning of a particular content.\n",
    "   - `[PAD]` (padding): When training LLMs with batch sizes larger than one, the input text in the batch might contain varying lengths. To ensure all texts have same length, we pad or extend the shorter texts with `PAD` token, upto the length of the longest text in the batch.\n",
    "\n",
    "<b>What happens if we don't pad input sequences?</b>\n",
    "- <span style=\"color:red\">Without padding, we cannot process multiple sequences in parallel (as batches) since neural networks expect fixed-size inputs:</span>\n",
    "  - Most deep learning frameworks require tensors with consistent dimensions for efficient computation\n",
    "  - The model's internal matrices and vectors are designed for fixed-size inputs\n",
    "  - Batched operations rely on regular shaped arrays/tensors\n",
    "- This would force us to process sequences one at a time, which would:\n",
    "  - Significantly slow down training and inference\n",
    "  - Prevent utilizing parallel processing capabilities of GPUs\n",
    "  - Increase computational costs\n",
    "- <span style=\"color:green\">With padding + attention masks, we can:\n",
    "  - Process variable length sequences efficiently in batches\n",
    "  - Tell the model to ignore padded tokens during attention computation\n",
    "  - Maintain the semantic meaning of the original sequences</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/add-special-tokens-to-vocabulary-to-deal-with-certain-contexts.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/appending-endoftext-token-to-independent-text-source.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed earlier, if the text is not in the vocabulary, the tokenization will fail. So, we need to add special tokens to the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1132\n",
      "\n",
      "Last 5 tokens in the vocabulary:\n",
      "('poverty', 1127)\n",
      "('landing', 1128)\n",
      "('mere', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "all_tokens = list((set(preprocessed)))\n",
    "all_tokens.extend(\n",
    "    [\n",
    "        \"<|endoftext|>\",  # Special token to indicate the end of a text sequence\n",
    "        \"<|unk|>\",  # Special token to indicate an unknown token/words (out-of-vocabulary words)\n",
    "    ]\n",
    ")\n",
    "vocab = {token: id for id, token in enumerate(all_tokens)}\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "print(\"\\nLast 5 tokens in the vocabulary:\")\n",
    "for item in list(vocab.items())[-5:]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We also need to adjust the tokenizer accordingly so that it knows when and how to use the new `<unk>` token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        # Store the vocabulary for access in encode and decode methods\n",
    "        self.str_to_int = vocab\n",
    "        # Create an inverse vocabulary that maps token ids to the original text tokens\n",
    "        self.int_to_str = {id: token for token, id in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess the input text into token ids\n",
    "        \"\"\"\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[token] for token in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        Convert token ids back to the original text\n",
    "        \"\"\"\n",
    "        text = \" \".join([self.int_to_str[id] for id in ids])\n",
    "\n",
    "        # Remove spaces before the specified punctuation marks\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r\"\\1\", text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to tokenize text with the modified tokenizer `SimpleTokenizerV2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join([text1, text2])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 402, 975, 1052, 1059, 3, 41, 1130, 987, 709, 727, 123, 428, 709, 1131, 274]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Byte pair encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What's Byte Pair Encoding (BPE), and where is it used?</b>\n",
    "- The BPE tokenizer is a subword tokenizer, which means it can split words into smaller parts.\n",
    "- The BPE tokenizer was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.\n",
    "  - Has a total vocabulary size of 50,257 tokens, with `<|<endoftex|>` being assigned the largest token ID.\n",
    "- <span style=\"color:green\"><b>The BPE tokenizer can handle any unknown words.</b></span>\n",
    "  - <span style=\"color:#4ea9fb\"><b>The ability to break down unknown words into subword tokens ensures that the tokenizer, and consequently the LLM, can process any text data, even if it contains words that were not present in the training data.</b></span>\n",
    "  \n",
    "<b>How does BPE work and handle unknown words?</b>\n",
    "- If the tokenizer encounters an unknown word, it can represent it as a sequence of subword tokens or characters.\n",
    "- For more details, refer https://prasanth.io/Knowledge/Tech/BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.8.0\n",
      "Available encoding names: ['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base', 'o200k_base']\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "print(f\"tiktoken version: {version('tiktoken')}\")\n",
    "\n",
    "encoding_list = tiktoken.list_encoding_names()\n",
    "print(f\"Available encoding names: {encoding_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details on different `tiktoken` encoding models, refer https://www.datacamp.com/tutorial/tiktoken-library-python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size for '<Encoding 'gpt2'>': 50257\n",
      "Vocabulary size for '<Encoding 'r50k_base'>': 50257\n",
      "Vocabulary size for '<Encoding 'p50k_base'>': 50281\n",
      "Vocabulary size for '<Encoding 'p50k_edit'>': 50284\n",
      "Vocabulary size for '<Encoding 'cl100k_base'>': 100277\n",
      "Vocabulary size for '<Encoding 'o200k_base'>': 200019\n"
     ]
    }
   ],
   "source": [
    "for tokenizer_temp in encoding_list:\n",
    "    tokenizer_temp = tiktoken.get_encoding(tokenizer_temp)\n",
    "    print(f\"Vocabulary size for '{tokenizer_temp}': {tokenizer_temp.n_vocab}\")\n",
    "del tokenizer_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the encoding for the model by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-2                         : <Encoding 'gpt2'>\n",
      "gpt-3.5                       : <Encoding 'cl100k_base'>\n",
      "gpt-4                         : <Encoding 'cl100k_base'>\n",
      "gpt-4o                        : <Encoding 'o200k_base'>\n",
      "gpt-4o-mini                   : <Encoding 'o200k_base'>\n",
      "text-embedding-3-small        : <Encoding 'cl100k_base'>\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    \"gpt-2\",\n",
    "    \"gpt-3.5\",\n",
    "    \"gpt-4\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o-mini\",\n",
    "    \"text-embedding-3-small\",\n",
    "]\n",
    "for model in models:\n",
    "    print(f\"{model:<30s}: {tiktoken.encoding_for_model(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_core_bpe', '_encode_bytes', '_encode_only_native_bpe', '_encode_single_piece', '_mergeable_ranks', '_pat_str', '_special_tokens', 'decode', 'decode_batch', 'decode_bytes', 'decode_bytes_batch', 'decode_single_token_bytes', 'decode_tokens_bytes', 'decode_with_offsets', 'encode', 'encode_batch', 'encode_ordinary', 'encode_ordinary_batch', 'encode_single_token', 'encode_with_unstable', 'eot_token', 'max_token_value', 'n_vocab', 'name', 'special_tokens_set', 'token_byte_values']\n"
     ]
    }
   ],
   "source": [
    "print(dir(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of items in BPE tokenizer vocabulary: 50257\n",
      "Special tokens: {'<|endoftext|>'} | {'<|endoftext|>': 50256}\n",
      "Largest token ID in the GPT-2 tokenizer: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(f\"No. of items in BPE tokenizer vocabulary: {tokenizer.n_vocab}\")\n",
    "print(f\"Special tokens: {tokenizer.special_tokens_set} | {tokenizer._special_tokens}\")\n",
    "print(\n",
    "    f\"Largest token ID in the GPT-2 tokenizer: {tokenizer.decode([tokenizer.n_vocab-1])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of tokens: 18 | No. of characters: 66\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea? In the sunlit terraces\" \"of someunknownPlace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(f\"No. of tokens: {len(integers)} | No. of characters: {len(text)}\")\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/tiktoken-verified-in-openai-tokenizer-sample.png\" width=\"800px\">\n",
    "\n",
    "Source:[https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of tokens: 20 | No. of characters: 80\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "    \"of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(f\"No. of tokens: {len(integers)} | No. of characters: {len(text)}\")\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\"><b>Exercise 2.1 Byte pair encoding of unknown words<br></b>\n",
    "Try the BPE tokenizer from the tiktoken library on the unknown words “Akwirw ier” and\n",
    "print the individual token IDs. Then, call the decode function on each of the resulting\n",
    "integers in this list to reproduce the mapping shown in figure 2.11. Lastly, call the\n",
    "decode method on the token IDs to check whether it can reconstruct the original\n",
    "input, “Akwirw ier.”\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/bpe-tokenizers-example.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Akwirw ier\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Akwirw ier\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Data sampling with a sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#4ea9fb\">After converting input text → tokens → token IDs, we need to generate the input-target (or input-output) pairs for training the LLM using <b>efficient data loaders</b><i> that iterates over the input dataset, and returns input-output pairs as PyTorch tensors (multidimensional arrays)</i></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/input-target-pairs-for-llm-training.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a data loader that fetches the input-target pairs (similar to the above image) from the training dataset using the slidinw window approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 20479\n",
      "Total tokens (in the training set): 5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "print(f\"Total characters: {len(raw_text)}\")\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(f\"Total tokens (in the training set): {len(enc_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the first 50 tokens from the dataset for demo as it results in slightly more interesting text passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of encoded text that's decoded:\n",
      "  and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
      "\n",
      "\"The height of his glory\"--that was what the women called it. I can hear Mrs. Gideon Th\n"
     ]
    }
   ],
   "source": [
    "enc_sample = enc_text[50:]\n",
    "print(f\"Sample of encoded text that's decoded:\\n {tokenizer.decode(enc_sample[:50])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#4ea9fb\"><b><code>context_size</code> determine how many tokens are included in the input text</b></span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating an input-output pair for next word prediction task:\n",
      "x: [290, 4920, 2241, 287]\n",
      "y: \t[4920, 2241, 287, 257]\n",
      "\n",
      "Creating multiple input ----> output pairs for next word prediction task:\n",
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n",
      "Corresponding text:\n",
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "# Create a input-output pair for next word prediction task.\n",
    "context_size = 4  # Determines how many tokens are included in the input text\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1 : context_size + 1]\n",
    "print(f\"Creating an input-output pair for next word prediction task:\")\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: \\t{y}\")\n",
    "print(\"\")\n",
    "\n",
    "print(f\"Creating multiple input ----> output pairs for next word prediction task:\")\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(f\"{context} ----> {desired}\")\n",
    "print(f\"Corresponding text:\")\n",
    "for i in range(1, context_size + 1):\n",
    "    context = tokenizer.decode(enc_sample[:i])\n",
    "    desired = tokenizer.decode(enc_sample[i : i + 1])\n",
    "    print(f\"{context} ----> {desired}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/data-lodaer-sample-text.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's create dataset and dataloader that extracts chunks of text using a sliding window approach from the input text dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:white; background-color:#4ea9fb; padding:15px\"><b>Listing 2.5 A dataset for batched inputs and targets<br></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        \"\"\"\n",
    "        Use sliding window to chunk the tokenized input dataset into overlapping input-output sequences of max_length (a.k.a context_size)\n",
    "\n",
    "        Args:\n",
    "        - txt: The input text a.k.a the training dataset\n",
    "        - tokenizer: The tokenizer object\n",
    "        - max_length: The number of tokens in the input text (a.k.a. context_size)\n",
    "        - stride: The number of tokens to move the window by (a.k.a. step_size). In other words, the stride determines how much the window moves to the right after each input-output pair is created.\n",
    "        \"\"\"\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)  # Tokenizes the entire text\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            output_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(output_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of input-output pairs (rows) in the dataset\"\"\"\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns a single row from dataset\"\"\"\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:white; background-color:#4ea9fb; padding:15px\"><b>Listing 2.6 A dataloader to generate batches with input-target pairs</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(\n",
    "    txt,\n",
    "    batch_size=4,\n",
    "    max_length=128,\n",
    "    stride=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a DataLoader object for the GPTDatasetV1 class\n",
    "\n",
    "    Args:\n",
    "    - txt: The input text a.k.a the training dataset\n",
    "    - batch_size: The number of input-output pairs to include in each batch\n",
    "    - max_length: The number of tokens in the input text (a.k.a. context_size or input_size)\n",
    "    - stride: The number of tokens to move the window by (a.k.a. step_size). In other words, the stride determines how much the window moves to the right after each input-output pair is created.\n",
    "    - shuffle: Whether to shuffle the data or not\n",
    "    - drop_last: Whether to drop the last incomplete batch or not\n",
    "    - num_workers: The number of CPU processes to use for pre-processing the data.\n",
    "    \"\"\"\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(\n",
    "        txt,\n",
    "        tokenizer,\n",
    "        max_length,\n",
    "        stride,\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,  # drop the last batch if it's smaller than the specified batch_size to prevent loss spikes during training.\n",
    "        num_workers=num_workers,  # The number of CPU processes to use for pre-processing the data.\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the `dataloader`  with `batch_size=1`, `max_length=4` (a.k.a. context_size) and `stride=1` to develop an intuition on how `GPTDatasetV1` class and `create_data_loader_v1` function works together.\n",
    "\n",
    "Note: \n",
    "- `max_length=4` is quite small, and only chosen for demonstration purposes. <span style=\"color:#4ea9fb\">It's common to train LLMs with input sizes of 256.</span>\n",
    "- `batch_size=1` is also chosen for demonstration purposes. <span style=\"color:red\">Small batch sizes require less memory during training, but lead to more noisy model updates.</span><span style=\"color:#4ea9fb\"> In practice, we use larger batch sizes to speed up training, and <b>the batch size is a tradeoff and hyperparameter to experiment with when training LLMs</b> , just like in regular deep learning.</span> \n",
    "- `stride=1` is also chosen for demonstration purposes. <span style=\"color:#4ea9fb\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch:\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "Second batch:\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "print(f\"First batch:\\n{first_batch}\")  # (input_ids, target_ids)\n",
    "second_batch = next(data_iter)\n",
    "print(f\"Second batch:\\n{second_batch}\")  # (input_ids, target_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\"><b>Exercise 2.2 Data loaders with different strides and context sizes<br></b>\n",
    "To develop more intuition for how the data loader works, try to run it with different\n",
    "settings such as <code>max_length=2</code> and <code>stride=2</code>, and <code>max_length=8</code> and <code>stride=2</code>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch:\n",
      "[tensor([[  40,  367],\n",
      "        [2885, 1464]]), tensor([[ 367, 2885],\n",
      "        [1464, 1807]])]\n",
      "Second batch:\n",
      "[tensor([[1807, 3619],\n",
      "        [ 402,  271]]), tensor([[ 3619,   402],\n",
      "        [  271, 10899]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    text, batch_size=2, max_length=2, stride=2, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "print(f\"First batch:\\n{first_batch}\")  # (input_ids, target_ids)\n",
    "second_batch = next(data_iter)\n",
    "print(f\"Second batch:\\n{second_batch}\")  # (input_ids, target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch:\n",
      "[tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],\n",
      "        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138]]), tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899],\n",
      "        [ 1464,  1807,  3619,   402,   271, 10899,  2138,   257]])]\n",
      "Second batch:\n",
      "[tensor([[ 1807,  3619,   402,   271, 10899,  2138,   257,  7026],\n",
      "        [  402,   271, 10899,  2138,   257,  7026, 15632,   438]]), tensor([[ 3619,   402,   271, 10899,  2138,   257,  7026, 15632],\n",
      "        [  271, 10899,  2138,   257,  7026, 15632,   438,  2016]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    text, batch_size=2, max_length=8, stride=2, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "print(f\"First batch:\\n{first_batch}\")  # (input_ids, target_ids)\n",
    "second_batch = next(data_iter)\n",
    "print(f\"Second batch:\\n{second_batch}\")  # (input_ids, target_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/sliding-window-sample-with-stride-of-1-and-4.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Target tensor:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# Another example\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(f\"Input tensor:\\n {inputs}\")\n",
    "print(f\"Target tensor:\\n {targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Note:</i> \n",
    "- By setting the `stride=4` same as `max_length=4`, <span style=\"color:green\">we utilize the dataset fully (as we don't skip a single word). At the same time, we don't have any overlap between the batches</span> as <span style=\"color:red\">having more overlap could lead to increased overfitting</span>.\n",
    "- <span style=\"color:red\">The input and output tensors we see above are token IDs, and are not the final input embeddings that are fed into the LLM</span>. The input embeddings are created by converting the token IDs into token embeddings and (optional but recommended) added with positional embeddings.\n",
    "  - i.e., $\\text{Token IDs} \\neq \\text{Token embeddings}$\n",
    "  - i.e., $\\text{Token embeddings} + \\text{Positional embeddings} = \\text{Input embeddings}$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Creating token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/creating-token-embeddings-flow-chart.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step in preparing input text for LLM is to <span style=\"color:#4ea9fb\"><i>convert token IDs into embeddings vectors</i></span>. \n",
    "- As a preliminary step, we need to <b>initialize these embeddings with random values</b>. The initialization serves as a starting point for the model to learn the embeddings (i.e., optimized) during training.\n",
    "- <span style=\"color:#4ea9fb\"><b><i>The embedding layer is essentially a lookup operation</i> that maps token IDs to embedding vectors. In other words, it retrieves rows from the embedding layer's weight matrix based on the token IDs.</b></span>\n",
    "  - <span style=\"color:#4ea9fb\">Each row in the output matrix is obtained via the lookup operation from the embedding layer's weight matrix.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's see how token ID to embedding vector conversion works with a simple example (`input_ids = [2,3,5,1]` after tokenization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ... | Reality | Demo |\n",
    "| - | - | - |\n",
    "| Vocabulary | 50,257 words in BPE tokenizer | 6 words |\n",
    "| Embedding size | 12,888 in GPT-3 | 3 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Input tensor: tensor([2, 3, 5, 1])\n",
      "\n",
      "2. Embedding layer's weigth matrix\n",
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "\n",
      "3a. Embedding vector for a single token ID = 3: \n",
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "3b. Embedding vector for all 4 input token IDs = tensor([2, 3, 5, 1]): \n",
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])  # => context_size or max_length = 4\n",
    "print(f\"1. Input tensor: {input_ids}\")\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "# Instantiate an embedding layer\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)  # 6 x 3\n",
    "print(f\"\\n2. Embedding layer's weigth matrix\")\n",
    "print(embedding_layer.weight)\n",
    "\n",
    "# Apply it to token ID to obtain the corresponding embedding vector of the token\n",
    "print(\n",
    "    f\"\\n3a. Embedding vector for a single token ID = 3: \\n{embedding_layer(torch.tensor([3]))}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\n3b. Embedding vector for all 4 input token IDs = {input_ids}: \\n{embedding_layer(input_ids)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/create-token-embeddings-my-own-viz.webp\" width=\"700px\">\n",
    "<img src=\"../images/emedding-layer-lookup-operation-simple-example.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#4ea9fb\">The above embedding layer approach is essentially <b>a more efficient way</b> of implementing one hot encoding followed by matrix multiplication in a fully connected layer.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that *we created embedding vectors from token IDs*, let's add a small modification to the embedding vectors to <span style=\"color:#4ea9fb\">encode positional information aboout a token within a text</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Encoding word positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">A minor shortcoming of LLM is that their <b>self attention mechanism (covered in chapter 3) doesn't have a notion of position of order for the tokens within a sequence</b>.</span>\n",
    "- <span style=\"color:red\">The way the embedding layer introduced in the above section works is that <b> the same token ID will always produce the same embedding vector, regardless of its position in the input text</b></span>  as shown in figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/embedding-layer-without-positional-awareness.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, the deterministic, position-independent embedding of the token ID is good for reproducability. However, <span style=\"color:red\"> since the self-attention mechanism of LLMs itself is also position-agnostic</span>, <span style=\"color:green\">it's beneficial to inject additional positional information into the embeddings.</spam>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What are the two broad categories of position-aware embeddings?</b>\n",
    "1. <b>Relative positional embeddings (RPE):</b>\n",
    "   - Instead of focusing on the absolute position of a token in the text, RPEs consider the relative position (or distance) of a token with respect to other tokens in the text. <span style=\"color:#4ea9fb\">In RPE, the model learns the relationships interms of <b>\"how far apart\"</b>, whereas in APE, the model learns the relationships in terms of <b>\"at which exact position\"</b>.</span>.\n",
    "   - The advantage is that the <span style=\"color:green\">model can generalize better to input sequences of varying lengths, even if it has not seen such lengths during training</span>.\n",
    "2. <b>Absolute positional embeddings (APE):</b>\n",
    "   - APE are directly associated with specific (absolute) positions in the input text. \n",
    "   - For each position in the input text, a unique embedding is added to the token's embedding to convey its exact position in the text.\n",
    "   - <span style=\"color:green\">OpenAI's GPT models use APE that are optimized during training process</span>, <span style=\"color:red\"> rather than being fixed or predefined like the positional encodings in the original transformer model.\n",
    "\n",
    "<b>What are the benefits of considering position-aware embeddings?</b>\n",
    "- Both type of positional embedddings aim to <span style=\"color:green\">augment the capacity of LLMs to understand the order and relationships between tokens, ensuring more accurate context-aware predictions.</span> \n",
    "- <span style=\"color:#4ea9fb\">The choice between RPE and APE depends on the specific task and the nature of the text data</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/positional-embeddings-added-to-token-embeddings-example.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's consider a more realistic embeddings size and encode input tokens into 256-dimensional embeddings (vector representations), with an additional 256-dimensional positional embeddings.\n",
    "\n",
    "| ... | Reality | Previous example | New example |\n",
    "| - | - | - | - |\n",
    "| Vocabulary | 50,257 words in BPE tokenizer | 6 words | 50257 words (assume token IDs are created by BPE tokenier) |\n",
    "| Embedding size | 12,888 in GPT-3 | 3 | 256 (smaller than original GPT-3, but reasonable for demonstration purposes) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor (Token IDs):\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "→ Inputs shape: \t\t\ttorch.Size([8, 4])\n",
      "\n",
      "→ Token embedding layer shape:\t\tEmbedding(50257, 256)\n",
      "\n",
      "→ Token embeddings shape: \t\ttorch.Size([8, 4, 256])\n",
      "\n",
      "→ Positional encoding layer shape:\tEmbedding(4, 256)\n",
      "\n",
      "→ Positional embeddings shape: \t\ttorch.Size([4, 256])\n",
      "\n",
      "→ Input embeddings shape: \t\ttorch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "### === DataLoader === ###\n",
    "max_length = 4  # context_size\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=8,\n",
    "    max_length=max_length,\n",
    "    stride=max_length,  # No overlap between input-output pairs\n",
    "    shuffle=False,\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(f\"Input tensor (Token IDs):\\n {inputs}\")  # 8 x 4 (batch_size x max_length)\n",
    "print(f\"\\n→ Inputs shape: \\t\\t\\t{inputs.shape}\")\n",
    "\n",
    "\n",
    "### === Token Embedding Layer === ###\n",
    "vocab_size = 50257\n",
    "output_dim = 256  # embedding_size\n",
    "token_embedding_layer = torch.nn.Embedding(\n",
    "    vocab_size, embedding_size\n",
    ")  # 50257 words, 256 dimensions\n",
    "print(f\"\\n→ Token embedding layer shape:\\t\\t{token_embedding_layer}\")\n",
    "\n",
    "### === Token Embeddings === ###\n",
    "# ❗ Use the token embedding layer to obtain the token embeddings (i.e., embed the input token IDs into 256-dimensional vectors)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\n",
    "    f\"\\n→ Token embeddings shape: \\t\\t{token_embeddings.shape}\"\n",
    ")  # 8 x 4 x 256 (batch_size x max_length x embedding_size)\n",
    "\n",
    "\n",
    "### === Positional Embedding Layer === ###\n",
    "# ❗ For GPT model's APE approach, we need to create another embedding layer that has same embedding dimension as the `token_embedding_layer`\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)  # 4 x 256\n",
    "print(f\"\\n→ Positional encoding layer shape:\\t{pos_embedding_layer}\")\n",
    "\n",
    "### === Positional Embeddings === ###\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))  # 4 x 256\n",
    "print(f\"\\n→ Positional embeddings shape: \\t\\t{pos_embeddings.shape}\")\n",
    "\n",
    "\n",
    "### === Add Positional Embeddings to Token Embeddings === ###\n",
    "input_embeddings = (\n",
    "    token_embeddings + pos_embeddings\n",
    ")  # 8 x 4 x 256 + 4 x 256 (broadcasting) => 8 x 4 x 256\n",
    "print(f\"\\n→ Input embeddings shape: \\t\\t{input_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's do a quick sanity check of **token embeddings** on the first token IDs in the batch. \n",
    "  - From the results below, it's evident once again that the embedding layer is essentially a lookup operation that maps token IDs to embedding vectors.\n",
    "  - Each token ID is now embedded into a 256-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embeddings for the first item in the batch (manual):\n",
      "tensor([[ 0.4247,  0.6801, -0.8078,  ...,  0.1753, -0.6280, -1.5333],\n",
      "        [ 0.5758,  1.0052, -0.5642,  ..., -0.1103,  2.8701,  1.1953],\n",
      "        [ 0.4584,  1.3983, -1.3585,  ...,  0.2204,  0.4413, -0.7308],\n",
      "        [-0.2103,  0.0598,  0.2861,  ...,  0.6133, -0.1733, -0.5151]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Token embeddings for the first item in the batch (automated):\n",
      "tensor([[ 0.4247,  0.6801, -0.8078,  ...,  0.1753, -0.6280, -1.5333],\n",
      "        [ 0.5758,  1.0052, -0.5642,  ..., -0.1103,  2.8701,  1.1953],\n",
      "        [ 0.4584,  1.3983, -1.3585,  ...,  0.2204,  0.4413, -0.7308],\n",
      "        [-0.2103,  0.0598,  0.2861,  ...,  0.6133, -0.1733, -0.5151]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token embeddings for the first item in the batch (manual):\")\n",
    "print(token_embedding_layer(torch.tensor([40, 367, 2885, 1464])))\n",
    "\n",
    "print(f\"\\nToken embeddings for the first item in the batch (automated):\")\n",
    "print(token_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's do a quick sanity check of **positional embeddings**. \n",
    "  - From the results below, it's evident that the positional embeddings are unique for each position in the input text.\n",
    "  - Each position is now embedded into a 256-dimensional vector.\n",
    "  - <span style=\"color:#4ea9fb\">Position embedding tensor consists of 4 position embeddings, each of size 256 (<code>output_dim</code>), corresponding to the 4 tokens (<code>context_length</code>) in the input text. These 4 x 256 embeddings will be broadcasted to each of the 8 (<code>batch_size</code>) tokenized inputs in the batch.</span>\n",
    "- <span style=\"color:#4ea9fb\">As we know, the <code>context_length</code> represents the supported input size of the LLM. Here, we choose it similar to the <code>max_length</code> of the input text. In practice, the input text are longer than the <code>context_length</code>, in which case we have to truncate the input text.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Positional embeddings (manual):\n",
      "tensor([[-1.1197, -0.8287,  0.6786,  ..., -0.1717,  0.9193, -1.7804],\n",
      "        [ 0.0681, -0.8354, -0.6381,  ...,  0.9914,  1.2633,  0.0893],\n",
      "        [ 0.2216, -0.8501, -0.4013,  ...,  0.4249,  0.3531,  0.4431],\n",
      "        [-0.7114,  0.1228,  1.4046,  ...,  0.2221,  0.5282,  1.0612]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Positional embeddings (automated):\n",
      "tensor([[-1.1197, -0.8287,  0.6786,  ..., -0.1717,  0.9193, -1.7804],\n",
      "        [ 0.0681, -0.8354, -0.6381,  ...,  0.9914,  1.2633,  0.0893],\n",
      "        [ 0.2216, -0.8501, -0.4013,  ...,  0.4249,  0.3531,  0.4431],\n",
      "        [-0.7114,  0.1228,  1.4046,  ...,  0.2221,  0.5282,  1.0612]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nPositional embeddings (manual):\")\n",
    "print(\n",
    "    pos_embedding_layer(torch.arange(context_length))\n",
    ")  # [0, 1, 2, 3] as context_length = 4\n",
    "\n",
    "print(f\"\\nPositional embeddings (automated):\")\n",
    "print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#4ea9fb\">The <code>input_embeddings</code> tensor we created are used as the input for the main LLM layers</span>, which we will being impelmenting in next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/block-diagram-input-text-to-input-embeddings.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>LLMs require textual data to be converted into numerical vectors</b>, since <span style=\"color:red\">they cannot process raw text data directly</span>. <span style=\"color:#4ea9fb\"><b>Embeddings transform discrete data (like words or images) into continuous vectors</b></span> that can be processed by neural networks.\n",
    "- <b>Raw input text > Tokens > Token IDs</b>\n",
    "  - First step in processing raw text data is <span style=\"color:#4ea9fb\"><b>tokenization</b></span>, which involves breaking text into smaller units, such as words or subwords. These tokens are then converted into integer representations (<span style=\"color:#4ea9fb\"><b>token IDs</b></span>) using a vocabulary.\n",
    "  - <span style=\"color:#4ea9fb\"><b>Special tokens</b></span> like `[UNK]`, `<|endoftext|>`, `[EOS]`, `[BOS]`, and `[PAD]` are added to the vocabulary to enhance model's understanding and handle various contexts, such as unknown words or mark the beginning and end of unrelated text segments.\n",
    "  - BPE tokenization used for LLMs like GPT-2 and GPT-3 is a subword tokenizer that can efficiently handle unknown words by splitting them into subword tokens (units or individual characters).\n",
    "- <b>Token IDs > Input-Output pairs</b>\n",
    "  - <span style=\"color:#4ea9fb\"><b>Sliding window</b></span> approach is used to generate input-target pairs for training LLMs. This approach involves extracting chunks of text from the input text dataset using a sliding window.\n",
    "- <b>Embeddings</b>\n",
    "  - Embedding layers in PyTorch function as a lookup operation, retrieving embedding vectors based on token IDs. <span style=\"color:#4ea9fb\"><b>The resulting embedding vectors provide continuous representations of the input tokens, which is crucial for training deep learning modules like LLMs.</b></span>\n",
    "  - While token embeddings provide consistent vector representations for each token, <span style=\"color:red\"> they lack a sense of the token's position in the input text</span>. <span style=\"color:green\">To remedy this, two main types of positional embeddings exists</span>:\n",
    "    - Absolute positional embeddings (APE) directly encode the position of a token in the input text.\n",
    "    - Relative positional embeddings (RPE) consider the relative position of a token with respect to other tokens in the text.\n",
    "  - OpenAI's GPT models use APE, which are added to the token embedding vectors and are optimized during the model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3_10_build_llm_form_scratch",
   "language": "python",
   "name": "py_3_10_build_llm_form_scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
