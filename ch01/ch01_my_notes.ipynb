{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding larage language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional ML/NLP methods excelled at categorization tasks (e.g., email spam classification) and pattern recognition, but <span style=\"color:red\">underperformed in language tasks that demanded complex understanding and generation abilities</span>.\n",
    "\n",
    "When we say language models \"understand\", we mean they can processe and generate text coherently/remarkably, <span style=\"color:red\">not that they possess humna-like consciousness or comprehension</span>.\n",
    "\n",
    "While earlier NLP models were designed for specific tasks, <span style=\"color:green\">LLMs demonstrate a broader proficiency</span>.\n",
    "\n",
    "The success behing LLMs can be attributed to \n",
    "1. <span style=\"color:green\">Transformer architecture that underpins many LLMs</span>, and\n",
    "2. <span style=\"color:green\">The vast amountd of data they are trained on</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 What's an LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#4ea9fb\">A deep neural network designed to understand, generate, and respond to human like text.</span>\n",
    "\n",
    "**What does the \"large\" in \"large language model\" refers?**\n",
    "1. <span style=\"color:#4ea9fb\">**model's size**</span> in terms of the parameters (10s or 100s of billions) and \n",
    "2. <span style=\"color:#4ea9fb\">**immense dataset**</span> (in trillions)\n",
    "\n",
    "**What does LLM do?**\n",
    "- Essentially predict the next word in sequence.\n",
    "\n",
    "**Why next word prediction?**\n",
    "- It <span style=\"color:#4ea9fb\">harnesses the inherent sequential nature of language</span> to train models on understanding context, structure, and relationships within text.\n",
    "\n",
    "**What architecture does LLM utilize?**\n",
    "- **Transformers** \n",
    "  - <span style=\"color:green\">Allows them to pay selective attention to different parts of the input when making predictions.</span>\n",
    "\n",
    "LLMs are a.k.a. Generative Artificial Intelligence or *Generative AI* or GenAI or *GAI*, as they can *generate text*. \n",
    "\n",
    "<br>\n",
    "<img src=\"images/hierarchical-depiction-of-AI-ML-DL-LLM-GAI.png\" width=\"600px\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Applications of LLMs\n",
    "\n",
    "LLMs are invaluable in automating almost any task that involveds parsing and generating text, and their applications are virtually endless.\n",
    "\n",
    "**The topic that interests me the most are**\n",
    "- <span style=\"color:green\">Effective knowledge retrieval from vast volumns of text in specialized areas like medicine</span> or law.\n",
    "- Multi-class Classification with detailed reasoning and explainability.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Stages of building and using LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">ðŸ’¡ Coding an LLM from ground up is the best way to understand it's mechanics and limitations, which essentially equips us with knowledge for pretraining and fine-tuning existing ope source LLMs to our own domain-specific datasets or tasks.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why custom build LLMs?**\n",
    "- LLMs that are tailored for specific tasks (BloombergGPT specialised for finance, <span style=\"color:green\">LLMs tailored for medical Q&A</span>) - can generally outperform general-purpose LLMs (e.g., ChatGPT).\n",
    "- Offers several advantages\n",
    "  - Data privacy.\n",
    "  - Custom implementation (e.g., locally on customer devices reduce server related costs and latency).\n",
    "  - Complete developer autonomy/control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Two key stages in creating an LLM?**\n",
    "1. **Pre-training**\n",
    "   - <span style=\"color:#4ea9fb\">Initial phase where the model is trained on a large, diverse dataset (aka *raw* text) to <i>develop a broad understanding of the language</i> and predict the next workd in the text</span>.\n",
    "   - Why \"raw\" text?\n",
    "     - <span style=\"color:red\">Traditional ML/DL models requires labelled info when trained via conventional supervised learning</span>.\n",
    "     - On the other hand, <span style=\"color:#4ea9fb\">LLMs use **self-supervised learning**, where the model generates its own label (**self-labelling**) from the input data (a.k.a **pseudo labels**)</span>.\n",
    "   - These pre-trained models are also known as *base* or *foundation* models.\n",
    "   - Capabilities\n",
    "     - Text completion\n",
    "     - Few-short capabilities\n",
    "   - Example\n",
    "     - GPT-3 model (A precursor for ChatGPT).\n",
    "2. **Fine-tuning**\n",
    "   - <span style=\"color:#4ea9fb\">A process where the pre-trained model (a.k.a. foundation model) is specifically trained on a narrower dataset that's more specific to a particular tasks or domains.</span>\n",
    "   - Two most popular categories of fine-tuning\n",
    "     - Classification fine-tuning\n",
    "     - 2.1 Instruction fine-tuning\n",
    "  \n",
    "| | Instruction fine-tuning | Classification fine-tuning |\n",
    "| - | - | - |\n",
    "| Labelled data | Instruction and answer pairs. | Texts and associated class labels. |\n",
    "| Example | Query to translate a text (in english) ccompanied by the correctly translated text (in german). | Emails associated with \"spam\" and \"not spam\" labels. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/two-stages-in-llm-creation.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Introduction to transformer archiecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why \"Attention Is All You Need\"?**\n",
    "- <span style=\"color:#4ea9fb\"><b>Most model LLMs rely on the *transformer* architecture</b></span>, which is a deep neural network (DNN) architecture introduced in 2017 paper \"Attention Is All You Need\".\n",
    "- <span style=\"color:#4ea9fb\"><b>To understand the LLMs, we must first understand the original transformer</b></span>, which was developed for machine translation (English to German and French).\n",
    "\n",
    "**Two submodules of transformer:**\n",
    "1. Encoder\n",
    "2. Decoder\n",
    "\n",
    "| | Encoder | Decoder |\n",
    "| - | - | - |\n",
    "| Description | Processes input texts, and encodes it into numerical representations or vectors that captures the contextual information. | Takes the encoded vectors and generates the output text. |\n",
    "| Example: Translation | Encode the input text from source language into vectors. | Decode the vectors to generate output text in target language. | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/simplified-depiction-of-original-transformer-architecture.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key component of transformers and LLMs**\n",
    "- Both encoders and decoders consists of many layers connected by self-attention mechanism.\n",
    "- The <span style=\"color:#4ea9fb\"><b>self-attention mechanism</b> allows the model to weigh the importance of different words or tokens in a sequence relative to each other, thus allowing the model to capture long-range dependencies and contextual relationships</span>.\n",
    "\n",
    "**Later variants of transformer architecture**\n",
    "\n",
    "| BERT | GPT |\n",
    "| --- | --- |\n",
    "| Bidirectional encoder representations from transformers | Generative pretrained transformers |\n",
    "| Built upon original transformer's encoder submodule. | Built upon transformers decoder submodule. |\n",
    "| Specialize in masked word prediction. | Trained to perform text completion tasks. | \n",
    "| Strengths in text classification tasks such as<br>- Sentiment prediction<br>- Document categorization | Strengths in text generation tasks such as <br>- Machine translation<br>- Text summarization <br>- Fiction writing<br> - Writing computer code<br>... <br><br>In addition to text completion, <span style=\"color:green\">GPT-like LLMs show remarkable versatility in their capabilities <b>without needing retraining, fine-tuning, or model archiecture changes</b></span>, such as</span><br>- <span style=\"color:#4ea9fb\">Zero-shot learning tasks</span><br>- <span style=\"color:#4ea9fb\">Few-shot learning tasks</span>|\n",
    "| Used in twitter/X to detect toxic content.| ChatGPT, etc. | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images//visual-representation-of-transformer-encoder-and-decoder-modules.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<img src=\"images/gpt-like-llms-are-good-at-zero-shot-and-few-shot.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">ðŸ’¡ <b>Transformers vs. LLMs</b><br> Not all transformers are LLMs. <br>Not all LLMs are transformers.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Utilize large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">Pre-training LLMs requires access to significant resources and is very expensive</span>.\n",
    "- <span style=\"color:red\">GPT-3 pretraining cost is estimated ~ $4.6 million interms of cloud computing credits.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<img src=\"images/pretraining-dataset-of-popular-gpt-3-llm.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">ðŸ’¡ <b>GPT-3 dataset details</b><br>- \"Number of tokens\" col total ~499 billion, but the model was trained only on 300 billion tokens (â‡’ 60% of the original data).<br>- 410 billion tokens from CommonCrawl dataset requires ~570GB of storage (<i>perhaps, not a lot </i>ðŸ¤”).<br>- Later iterations of GPT-3 like models such as Meta's Llama expanded training scope to <br><tab>&nbsp;&nbsp;&nbsp;- Arxiv research papers (92GB).<br>&nbsp;&nbsp;&nbsp;- StackExchange's code-related Q&As (78 GB).<br>- GPT-3 paper authors did not share the training dataset.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6 A closer look at the GPT architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT was originally introduced in the paper \"Improving Language Understanding by Generative Pre-Training\" [paper](https://mng.bz/x2qg) from OpenAI.\n",
    "- ChatGPT was created by finetuning GPT-3 on a large instruction dataset using a method from OpenAI's InstructGPT [paper](https://arxiv.org/abs/2203.02155)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<img src=\"images/next-word-prediction-model-of-gpt-modules.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Perks of next word prediction tasks in training LLMs**\n",
    "- <span style=\"color:#4ea9fb\">the **next-word prediction** is a form of **self-supervised learning**, which is a form of **self-labeling**</span>.\n",
    "- <span style=\"color:#4ea9fb\">we can **create labels \"on the fly\"**, thus allowing us to **use massive unlabelled text datasets** for training LLMs</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<b>What is an <i>auto-regressive</i> model</b>?\n",
    "- <span style=\"color:#4ea9fb\">Models that incorporate previous outpus as inputs for future predictions</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Why GPT-like models are considered as a <i>auto-regressive</i> models?</b>\n",
    "- GPT just uses the decoder part of the transformer architecture without the encoder.\n",
    "  - <span style=\"color:#4ea9fb\">It's designed for <b>unidirectional, left-to-right processing</b></span>.\n",
    "- Since <span style=\"color:#4ea9fb\">decoder only models like GPT generate text by predicting output text one word at a time in a iterative fashion</span>, they are considerde as <i>autoregressive</i> models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<b>What's an <i>emergent behaviour</i> or <i>emergent abilities</i>?</b>\n",
    "- <span style=\"color:#4ea9fb\"><b>Ability to perform tasks the model was not explicilty trained to perform.</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benefits and capabilities of large scale, generative models**\n",
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">&nbsp;&nbsp;&nbsp;ðŸ’¡Though the GPT models were not specifically trained to perform translation task (unlike the original transformers), and rather primarily trained on next-word prediction task, it emerges as a natural consequence of the model's exposure to vast quantities of multilingual dataset in diverse contexts.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 1.7 Building a large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<b>3 main stage of coding/building an LLM</b>\n",
    "1. **Building an LLM:** Learn the fundamentals of data preproccessing steps + code the attenttion mechanism + LLM architecture.\n",
    "2. **Foundation model**: Code & pre-train a GPT-like LLM + Model evaluation + loading openly available model weights.\n",
    "3. **Fine-tuned model**: Fine-tuning to follow instructions: Classifier + Personal assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LLMs have **transformed the field of NLP**, leading to advancements in understanding, generating, and translating human language.\n",
    "- Modern LLMs are trained in two main steps:\n",
    "  1. **Pre-training**: On a large corpus of unlabelled text in self-superivsing fashion by using the prediction of the next word in a sentence as a label (psuedo-label or self-label).\n",
    "  2. **Fine-tuning**: The base/foundational/pre-trained model on smaller/narrower, labelled dataset to follow instructions or perform classification tasks.\n",
    "- **LLMs are based on transformer architecture**. The key idea in transformer architecture is **\"attention mechanism\"** that allows the model to weigh the importance of each word in a sequence relative to other words, thus allowing the model to capture longrange contextual understandings and relationships/dependencies.\n",
    "- Original transformer architecture consists of two submodules\n",
    "    1. **Encoder**: For encoding input text into numerical vector representations that captures the contectual understanding\n",
    "    2. **Decoder**: For decoding the encoded vectors into target language.\n",
    "- LLMs trained for text generative tasks (like GPT-3 and ChatGPT) only uses decoder submodule of the transformer architecture.\n",
    "- LLMs are pre-trained on** massive unlabelled raw datasets **(e.g., Trillions of tokens).\n",
    "- LLMs exhibit **emergent abilities**, such as capabilities to classify, translate, and summarize text.\n",
    "  - ðŸ’¡Though the GPT models were not specifically trained to perform translation task (unlike the original transformers), and rather primarily trained on next-word prediction task, it emerges as a natural consequence of the model's exposure to vast quantities of multilingual dataset in diverse contexts.\n",
    "-** LLMs finetuned on custom datasets can outperform general LLMs on specific tasks**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
