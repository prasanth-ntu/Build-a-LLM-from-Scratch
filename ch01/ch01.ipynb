{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional ML/NLP methods excelled at categorization tasks (e.g., email spam classification) and pattern recognition, but <span style=\"color:red\">underperformed in language tasks that demanded complex understanding and generation abilities</span>.\n",
    "\n",
    "When we say language models \"understand\", we mean they can processe and generate text remarkably, <span style=\"color:red\">not that they possess humna-like consciousness or comprehension</span>.\n",
    "\n",
    "While earlier NLP models were designed for specific tasks, <span style=\"color:green\">LLMs demonstrate a broader proficiency</span>.\n",
    "\n",
    "The success behing LLMs can be attributed to \n",
    "1. <span style=\"color:green\">Transformer architecture that underpins many LLMs</span>, and\n",
    "2. <span style=\"color:green\">The vast amountd of data they are trained on</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 What's an LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#4ea9fb\">A deep neural network designed to understand, generate, and respond to human like text.</span>\n",
    "\n",
    "**What does the **\"large\" in \"large language model\"** refers?**\n",
    "1. <span style=\"color:#4ea9fb\">**model's size**</span> in terms of the parameters (10s or 100s of billions) and \n",
    "2. <span style=\"color:#4ea9fb\">A**immense dataset**</span> (in trillions)\n",
    "\n",
    "**What does LLM do?**\n",
    "- Essentially predict the next word in sequence.\n",
    "\n",
    "**Why next word prediction?**\n",
    "- It <span style=\"color:#4ea9fb\">harnesses the inherent sequential nature of language</span> to train models on understanding context, structure, and relationships within text.\n",
    "\n",
    "**What architecture does LLM utilize?**\n",
    "- **Transformers** \n",
    "  - <span style=\"color:green\">Allows them to pay selective attention to different parts of the input when making predictions.</span>\n",
    "\n",
    "LLMs are a.k.a. Generative Artificial Intelligence or *Generative AI* or GenAI or *GAI*, as they can *generate text*. \n",
    "\n",
    "<br>\n",
    "<img src=\"images/hierarchical-depiction-of-AI-ML-DL-LLM-GAI.png\" width=\"600px\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Applications of LLMs\n",
    "\n",
    "LLMs are invaluable in automating almost any task that involveds parsing and generating text, and their applications are virtually endless.\n",
    "\n",
    "**The topic that interests me the most are**\n",
    "- <sapn style=\"color:green\">Effective knowledge retrieval from vast volumns of text in specialized areas like medicine</span> or law.\n",
    "- Multi-class Classification with detailed reasoning and explainability.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Stages of building and using LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">ðŸ’¡ Coding an LLM from ground up is the best way to understand it's mechanics and limitations, which essentially equips us with knowledge for pretraining and fine-tuning existing ope source LLMs to our own domain-specific datasets or tasks.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why custom build LLMs?**\n",
    "- LLMs that are tailored for specific tasks (BloombergGPT specialised for finance, <span style=\"color:green\">LLMs tailored for medical Q&A</span>) - can generally outperform general-purpose LLMs (e.g., ChatGPT).\n",
    "- Offers several advantages\n",
    "  - Data privacy.\n",
    "  - Custom implementation (e.g., locally on customer devices reduce server related costs and latency).\n",
    "  - Complete developer autonomy/control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Two key stages in creating an LLM?**\n",
    "1. **Pre-training**\n",
    "   - <span style=\"color:#4ea9fb\">Initial phase where the model is trained on a large, diverse dataset (aka *raw* text) to <i>develop a broad understanding of the language</i> and predict the next workd in the text</span>.\n",
    "   - Why \"raw\" text?\n",
    "     - <span style=\"color:red\">Traditional ML/DL models requires labelled info when trained via conventional supervised learning</span>.\n",
    "     - On the other hand, <span style=\"color:#4ea9fb\">LLMs use self-supervised learning, where the model generates its own label from the input data (a.k.a pseudo labels)</span>.\n",
    "   - These pre-trained models are also known as *base* or *foundation* models.\n",
    "   - Capabilities\n",
    "     - Text completion\n",
    "     - Few-short capabilities\n",
    "   - Example\n",
    "     - GPT-3 model (A precursor for ChatGPT).\n",
    "2. **Fine-tuning**\n",
    "   - <span style=\"color:#4ea9fb\">A process where the pre-trained model (a.k.a. foundation model) is specifically trained on a narrower dataset that's more specific to a particular tasks or domains.</span>\n",
    "   - Two most popular categories of fine-tuning\n",
    "     - Classification fine-tuning\n",
    "     - 2.1 Instruction fine-tuning\n",
    "  \n",
    "| | Instruction fine-tuning | Classification fine-tuning\n",
    "| - | - | - |\n",
    "| Labelled data | Instruction and Answer pairs. | Texts and associated class labels. |\n",
    "| Example | Query to translate a text accompanied by the correctly translated text. | Emails associated with \"spam\" and \"not spam\" labels. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/two-stages-in-llm-creation.png\" width=\"600px\">\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
