{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional ML/NLP methods excelled at categorization tasks (e.g., email spam classification) and pattern recognition, but <span style=\"color:red\">underperformed in language tasks that demanded complex understanding and generation abilities</span>.\n",
    "\n",
    "When we say language models \"understand\", we mean they can processe and generate text remarkably, <span style=\"color:red\">not that they possess humna-like consciousness or comprehension</span>.\n",
    "\n",
    "While earlier NLP models were designed for specific tasks, <span style=\"color:green\">LLMs demonstrate a broader proficiency</span>.\n",
    "\n",
    "The success behing LLMs can be attributed to \n",
    "1. <span style=\"color:green\">Transformer architecture that underpins many LLMs</span>, and\n",
    "2. <span style=\"color:green\">The vast amountd of data they are trained on</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 What's an LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#4ea9fb\">A deep neural network designed to understand, generate, and respond to human like text.</span>\n",
    "\n",
    "**What does the **\"large\" in \"large language model\"** refers?**\n",
    "1. <span style=\"color:#4ea9fb\">**model's size**</span> in terms of the parameters (10s or 100s of billions) and \n",
    "2. <span style=\"color:#4ea9fb\">A**immense dataset**</span> (in trillions)\n",
    "\n",
    "**What does LLM do?**\n",
    "- Essentially predict the next word in sequence.\n",
    "\n",
    "**Why next word prediction?**\n",
    "- It <span style=\"color:#4ea9fb\">harnesses the inherent sequential nature of language</span> to train models on understanding context, structure, and relationships within text.\n",
    "\n",
    "**What architecture does LLM utilize?**\n",
    "- **Transformers** \n",
    "  - <span style=\"color:green\">Allows them to pay selective attention to different parts of the input when making predictions.</span>\n",
    "\n",
    "LLMs are a.k.a. Generative Artificial Intelligence or *Generative AI* or GenAI or *GAI*, as they can *generate text*. \n",
    "\n",
    "<br>\n",
    "<img src=\"images/hierarchical-depiction-of-AI-ML-DL-LLM-GAI.png\" width=\"600px\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Applications of LLMs\n",
    "\n",
    "LLMs are invaluable in automating almost any task that involveds parsing and generating text, and their applications are virtually endless.\n",
    "\n",
    "**The topic that interests me the most are**\n",
    "- <sapn style=\"color:green\">Effective knowledge retrieval from vast volumns of text in specialized areas like medicine</span> or law.\n",
    "- Multi-class Classification with detailed reasoning and explainability.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Stages of building and using LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">ðŸ’¡ Coding an LLM from ground up is the best way to understand it's mechanics and limitations, which essentially equips us with knowledge for pretraining and fine-tuning existing ope source LLMs to our own domain-specific datasets or tasks.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why custom build LLMs?**\n",
    "- LLMs that are tailored for specific tasks (BloombergGPT specialised for finance, <span style=\"color:green\">LLMs tailored for medical Q&A</span>) - can generally outperform general-purpose LLMs (e.g., ChatGPT).\n",
    "- Offers several advantages\n",
    "  - Data privacy.\n",
    "  - Custom implementation (e.g., locally on customer devices reduce server related costs and latency).\n",
    "  - Complete developer autonomy/control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Two key stages in creating an LLM?**\n",
    "1. **Pre-training**\n",
    "   - <span style=\"color:#4ea9fb\">Initial phase where the model is trained on a large, diverse dataset (aka *raw* text) to <i>develop a broad understanding of the language</i> and predict the next workd in the text</span>.\n",
    "   - Why \"raw\" text?\n",
    "     - <span style=\"color:red\">Traditional ML/DL models requires labelled info when trained via conventional supervised learning</span>.\n",
    "     - On the other hand, <span style=\"color:#4ea9fb\">LLMs use self-supervised learning, where the model generates its own label from the input data (a.k.a pseudo labels)</span>.\n",
    "   - These pre-trained models are also known as *base* or *foundation* models.\n",
    "   - Capabilities\n",
    "     - Text completion\n",
    "     - Few-short capabilities\n",
    "   - Example\n",
    "     - GPT-3 model (A precursor for ChatGPT).\n",
    "2. **Fine-tuning**\n",
    "   - <span style=\"color:#4ea9fb\">A process where the pre-trained model (a.k.a. foundation model) is specifically trained on a narrower dataset that's more specific to a particular tasks or domains.</span>\n",
    "   - Two most popular categories of fine-tuning\n",
    "     - Classification fine-tuning\n",
    "     - 2.1 Instruction fine-tuning\n",
    "  \n",
    "| | Instruction fine-tuning | Classification fine-tuning |\n",
    "| - | - | - |\n",
    "| Labelled data | Instruction and answer pairs. | Texts and associated class labels. |\n",
    "| Example | Query to translate a text accompanied by the correctly translated text. | Emails associated with \"spam\" and \"not spam\" labels. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/two-stages-in-llm-creation.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Introduction to transformer archiecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why \"Attention Is All You Need\"?**\n",
    "- <span style=\"color:#4ea9fb\"><b>Most model LLMs rely on the *transformer* architecture</b></span>, which is a deep neural network (DNN) architecture introduced in 2017 paper \"Attention Is All You Need\".\n",
    "- <span style=\"color:#4ea9fb\"><b>To understand the LLMs, we must first understand the original transformer</b></span>, which was developed for machine translation (English to German and French).\n",
    "\n",
    "**Two submodules of transformer:**\n",
    "1. Encoder\n",
    "2. Decoder\n",
    "\n",
    "| | Encoder | Decoder |\n",
    "| - | - | - |\n",
    "| Description | Processes input texts, and encodes it into numerical representations or vectors that captures the contextual information. | Takes the encoded vectors and generates the output text. |\n",
    "| Example: Translation | Encode the input text from source language into vectors. | Decode the vectors to generate output text in target language. | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/simplified-depiction-of-original-transformer-architecture.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key component of transformers and LLMs**\n",
    "- Both encoders and decoders consists of many layers connected by self-attention mechanism.\n",
    "- The <span style=\"color:#4ea9fb\"><b>self-attention mechanism</b> allows the model to weigh the importance of different words or tokens in a sequence relative to each other, thus allowing the model to capture long-range dependencies and contextual relationships</span>.\n",
    "\n",
    "**Later variants of transformer architecture**\n",
    "\n",
    "| BERT | GPT |\n",
    "| - | - |\n",
    "| Bidirectional encoder representations from transformers | Generative pretrained transformers |\n",
    "| Built upon original transformer's encoder submodule. | Built upon transformers decoder submodule. |\n",
    "| Specialize in masked word prediction. | Trained to perform text completion tasks. | \n",
    "| Strengths in text classification tasks such as<br>- Sentiment prediction<br>- Document categorization | Strengths in text generation tasks such as <br>- Machine translation<br>- Text summarization <br>- Fiction writing<br> - Writing computer code<br>... <br><br>In addition to text completion, <span style=\"color:green\">GPT-like LLMs show remarkable versatility in their capabilities <b>without needing retraining, fine-tuning, or model archiecture changes</b></span>, such as</span><br>- <span style=\"color:#4ea9fb\">Zero-shot learning tasks</span><br>- <span style=\"color:#4ea9fb\">Few-shot learning tasks</span>|\n",
    "| Used in twitter/X to detect toxic content.| ChatGPT, etc. | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images//visual-representation-of-transformer-encoder-and-decoder-modules.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<img src=\"images/gpt-like-llms-are-good-at-zero-shot-and-few-shot.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">ðŸ’¡ <b>Transformers vs. LLMs</b><br> Not all tranformers are LLMs. <br>Not all LLMs are transformers.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilize large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">Pre-training LLMs requires access to significant resources and is very expensive</span>.\n",
    "- <span style=\"color:red\">GPT-3 pretraining cost is estimated ~ $4.6 million interms of cloud computing credits.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<img src=\"images/pretraining-dataset-of-popular-gpt-3-llm.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">ðŸ’¡ <b>GPT-3 dataset details</b><br>- \"Number of tokens\" col total ~499 billion, but the model was trained only on 300 billion tokens (â‡’ 60% of the original data).<br>- 410 billion tokens from CommonCrawl dataset requires ~570GB of storage (<i>perhaps, not a lot </i>ðŸ¤”).<br>- Later iterations of GPT-3 like models such as Meta's Llama expanded training scope to <br><tab>&nbsp;&nbsp;&nbsp;- Arxiv research papers (92GB).<br>&nbsp;&nbsp;&nbsp;- StackExchange's code-related Q&As (78 GB).<br>- GPT-3 paper authors did not share the training dataset.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
