{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<img src=\"../images/figure-3.1-three-main-stages-of-llm-chapter-3-focus-on-stage-1-step-2.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will look at <span style=\"color:#4ea9fb\"><b>attention mechanism</b> in isolation and focus on them at mechanistic level</span>.\n",
    "- We will implement <b>4 different variants of attention mechanisms</b>:\n",
    "  - <span style=\"color:#4ea9fb\"><b>Simplified self-attention</b></span>\n",
    "  - <span style=\"color:#4ea9fb\"><b>Self-attention with trainable weights</b></span>\n",
    "  - <span style=\"color:#4ea9fb\"><b>Casual attention</b></span> \n",
    "    - Adds mask to self-attention that allows the models to only considered previous and current inputs in a sequence.\n",
    "  - <span style=\"color:#4ea9fb\"><b>Multi-head attention</b></span>\n",
    "    - Organizes attention mechanism into multiple heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/figure-3.2-four-different-variants-of-attention-mechanism.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 The problem with modeling long sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, we are in <b>pre-LLM era</b>, and we want to <b>develop a language translation model</b>.\n",
    "- <span style=\"color:red\">We cannot simply translate a text word by word due to the grammatial structures of the source and target language.</span>\n",
    "- To address this problem, DNNs generally use two submodules, \n",
    "  - <span style=\"color:#4ea9fb\"><b>encoder</b></span> (first, read and process the entire text) and \n",
    "  - <span style=\"color:#4ea9fb\"><b>decoder</b></span> (then produces the translated text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/figure-3.3-german-to-english-problem-with-word-for-word-translation.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What's RNN, and why they were popular before transformers?</b>\n",
    "- Before the advent of transformers, <span style=\"color:green\"><i><b>recurrent neural networks </i>(RNNs) were the most popular encoder-decoder architecture for language translation</b></span>.\n",
    "- <span style=\"color:#4ea9fb\">RNN is a type of NN where outputs from the previous step are fed as inputs to the current step, making them suitable for sequential data like text.</span>\n",
    "\n",
    "<b>What does the RNN (encoder-decoder) do?</b>\n",
    "- <span style=\"color:#4ea9fb\">The encoder processes a sequence of words/tokens from source language as input, using a hidden state an intermediate neural network layer of the encoder&mdash;to generate a condensed (encoded) representation of the entire input sequence.</span>\n",
    "- <span style=\"color:#4ea9fb\">The decoder then uses this encoded representation (hidden state) to generate the translated text, one word at a time (i.e., token by token).</span>\n",
    "\n",
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">ðŸ’¡ <b>Key idea of encoder-decoder RNNs</b><br><b>- Encoder</b>: Processes the entire input text into hidden state (memory cell).<br><b>- Decoder</b>: Takes in this hidden state to produce the output, one word at a time.<br><b>- Hidden state:</b> ~ Embedding vector</span></p>\n",
    "\n",
    "<b>What's the problem with encoder-decoder RNNs?</b>\n",
    "- <span style=\"color:red\">RNNs have a hard time capturing long-range dependencies in the complex sentences.</span>\n",
    "  - RNN cannot directly access earlier hidden state from the encoder during the decoding phase.\n",
    "  - Consequently, the decoder relies solely on the current hidden state, which despite encapsulating all relevant information, may not be sufficient to generate the correct translation.\n",
    "  - This leads to loss of context.\n",
    "    - Although RNNs work fine for short sentences, <span style=\"color:red\">they struggle with longer sentences as they don't have direct access to previous words in the input sequence.</span>\n",
    "- <span style=\"color:#4ea9fb\">This motivated the design of attention mechanisms.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/figure-3.4-german-to-english-translation-using-RNNs-encoder-decoder.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Capturing data dependencies with attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Why attention mechanisms?</b>\n",
    "- <span style=\"color:red\">One major shortcomings of above RNNs is that it must remember the entire encoded input in a single hidden state before passing it to the decoder (figure 3.4).</span>\n",
    "- <span style=\"color:#4ea9fb\">Attention mechanisms address this issue by allowing the decoder to focus  on (i.e., selectively access) different parts of the input sequence at each decoding step (figure 3.5).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/figure-3.5-german-to-english-translation-using-RNNs-encoder-decoder-with-attention-mechanism.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3_10_build_llm_form_scratch",
   "language": "python",
   "name": "py_3_10_build_llm_form_scratch"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
