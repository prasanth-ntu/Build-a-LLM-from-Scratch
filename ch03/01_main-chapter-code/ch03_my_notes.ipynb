{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<img src=\"../images/figure-3.1-three-main-stages-of-llm-chapter-3-focus-on-stage-1-step-2.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will look at <span style=\"color:#4ea9fb\"><b>attention mechanism</b> in isolation and focus on them at mechanistic level</span>.\n",
    "- We will implement <b>4 different variants of attention mechanisms</b>:\n",
    "  - <span style=\"color:#4ea9fb\"><b>Simplified self-attention</b></span>\n",
    "  - <span style=\"color:#4ea9fb\"><b>Self-attention with trainable weights</b></span>\n",
    "  - <span style=\"color:#4ea9fb\"><b>Casual attention</b></span> \n",
    "    - Adds mask to self-attention that allows the models to only consider previous and current inputs in a sequence.\n",
    "  - <span style=\"color:#4ea9fb\"><b>Multi-head attention</b></span>\n",
    "    - Organizes attention mechanism into multiple heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/figure-3.2-four-different-variants-of-attention-mechanism.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 The problem with modeling long sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, we are in <b>pre-LLM era</b>, and we want to <b>develop a language translation model</b>.\n",
    "- <span style=\"color:red\">We cannot simply translate a text word by word due to the <b>grammatial structures and contextual understanding</b> of the source and target language.</span>\n",
    "- To address this problem, DNNs generally use two submodules, \n",
    "  - <span style=\"color:#4ea9fb\"><b>encoder</b></span> (first, read and process the entire text) and \n",
    "  - <span style=\"color:#4ea9fb\"><b>decoder</b></span> (then produces the translated text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/figure-3.3-german-to-english-problem-with-word-for-word-translation.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What's RNN, and why they were popular before transformers?</b>\n",
    "- Before the advent of transformers, <span style=\"color:green\"><i><b>recurrent neural networks </i>(RNNs) were the most popular encoder-decoder architecture for language translation</b></span>.\n",
    "- <span style=\"color:#4ea9fb\">RNN is a type of NN where outputs from the previous step are fed as inputs to the current step, making them suitable for sequential data like text.</span>\n",
    "\n",
    "<b>What does the RNN (encoder-decoder) do?</b>\n",
    "- <span style=\"color:#4ea9fb\">The encoder processes a sequence of words/tokens from source language as input, using a hidden state, an intermediate neural network layer of the encoder&mdash;to generate a condensed (encoded) representation of the entire input sequence.</span>\n",
    "- <span style=\"color:#4ea9fb\">The decoder then uses this encoded representation (hidden state) to generate the translated text, one word at a time (i.e., token by token).</span>\n",
    "\n",
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">üí° <b>Key idea of encoder-decoder RNNs</b><br><b>- Encoder</b>: Processes the entire input text into hidden state (memory cell).<br><b>- Decoder</b>: Takes in this hidden state to produce the output, one word at a time.<br><b>- Hidden state:</b> ~ Similar to Embedding vector in Chapter 2.</span></p>\n",
    "\n",
    "<b>What's the problem with encoder-decoder RNNs?</b>\n",
    "- <span style=\"color:red\">RNNs have a hard time capturing long-range dependencies in the complex sentences.</span>\n",
    "  - RNN cannot directly access earlier hidden state from the encoder during the decoding phase.\n",
    "  - Consequently, the decoder relies solely on the current hidden state, which despite encapsulating all relevant information, may not be sufficient to generate the correct translation.\n",
    "  - This leads to loss of context.\n",
    "    - Although RNNs work fine for short sentences, <span style=\"color:red\">they struggle with longer sentences as they don't have direct access to previous words in the input sequence.</span>\n",
    "- <span style=\"color:#4ea9fb\">This motivated the design of attention mechanisms.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/figure-3.4-german-to-english-translation-using-RNNs-encoder-decoder.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Capturing data dependencies with attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Why attention mechanisms?</b>\n",
    "- <span style=\"color:red\">One major shortcomings of above RNNs is that it must remember the entire encoded input in a single hidden state before passing it to the decoder (figure 3.4).</span>\n",
    "- <span style=\"color:#4ea9fb\">Attention mechanisms address this issue by allowing the decoder to focus  on (i.e., selectively access) different parts of the input sequence at each decoding step, implying that <b>certain input tokens hold more significance than others in the generation of a specific output token</b> (figure 3.5).</span>\n",
    "\n",
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">üí° Interestingly, <b>3 years after researchers developed <i>attention</i> mechanism for RNN, they found that RNN architectures are not required for DNN for NLP and proposed original <i>transformer</i> architecture.</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/figure-3.5-german-to-english-translation-using-RNNs-encoder-decoder-with-attention-mechanism.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Why self-attention?</b>\n",
    "- <span style=\"color:#4ea9fb\">It allows each position in the input sequence to <b>\"attend to\"</b> (i.e., compute relevancy of) all positions in the input sequence when computing the representation of the sequence.</span>\n",
    "- <span style=\"color:#4ea9fb\">Self-attention is a <b>key component of contemporary LLMs based on the transformer architecture</b>, such as BERT, GPT-2, and T5.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/figure-3.6-self-attention-mechanism-topic-of-the-current-chapter.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Attending to different part of the input with self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">üí°‚ùó Once we <b>grasp the fundamentals of self-attention</b>, we would have <b>conquered one of the toughest aspects of this book and LLM implementation</b> in general.</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">üí°<b>The \"self\" in self-attention.</b><br>\n",
    "- <span style=\"color:#4ea9fb\">\"Self\" in in self-attention refers to the mechanim's <b>ability to compute attention weights by relating different positions within a single input sequence</b></span>.<br>\n",
    "- It assess and <b>learns the relationships and dependencies between various parts of the input itself</b>, <i>such as words in a sentence</i> or <i>pixels in an image</i>.<br><br>\n",
    "- <span style=\"color:red\">This is in contrast to traditional attention mechanisms, where the focus is on the relationship between two different sequences, such as in sequence-to-sequence models (for machine translation) where the attention might be between an source (input) and target (output) sentences </span>(e.g., figure 3.5).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 A simple self-attention mechanism without trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">‚ö†Ô∏è This section is purely for illustration purposes and NOT the attention mechanism used in transformers.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR\n",
    "    A[Input embeddings] --> B[Attention scores]\n",
    "    B --> C[Attention weights]\n",
    "    C --> D[Context vectors]\n",
    "```\n",
    "\n",
    "[![](https://mermaid.ink/img/pako:eNpVz80OgjAMAOBXWXqWF-BgInAx8aQ3nYfJKiy6lozOnxDe3YEhxp768zVpB6jZIuTQBNO1arfXpFJsThq21EVR6C9oraOm13BWWbZWRZptRJDEMam-5oDT7LtYzKT8I090TSs_U86mSqZkEnyJemAtHCYBK_AYvHE23TRMXoO06FFDnlJrwk2DpjE5E4UPb6ohlxBxBYFj0y5F7KwRrJxJf_ml2Rk6Mqfyau49jh-9N1KY?type=png)](https://mermaid.live/edit#pako:eNpVz80OgjAMAOBXWXqWF-BgInAx8aQ3nYfJKiy6lozOnxDe3YEhxp768zVpB6jZIuTQBNO1arfXpFJsThq21EVR6C9oraOm13BWWbZWRZptRJDEMam-5oDT7LtYzKT8I090TSs_U86mSqZkEnyJemAtHCYBK_AYvHE23TRMXoO06FFDnlJrwk2DpjE5E4UPb6ohlxBxBYFj0y5F7KwRrJxJf_ml2Rk6Mqfyau49jh-9N1KY)\n",
    "\n",
    "$$x^{(i)} \\longrightarrow \\omega^{(i)} \\longrightarrow \\alpha^{(i)} \\longrightarrow z^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input sequence, $x^{(1)}$ to $x^{(T)}$ \n",
    "  - The input sequence is a text (for e.g., <i>\"Your journey starts with one step\"</i>) that has already been converted into token embeddings. \n",
    "    - For instance, $x^{(1)} = [0.4, 0.1, 0.8]$ is a $\\text{d}$-dimensional (3-dimensional) vector that represents the word <i>\"Your\"</i>. \n",
    "- Goal: In self-attention, our goal is to <span style=\"color:#4ea9fb\"><b>compute context vectors (a.k.a. enriched embedding vector) $z^{(i)}$  for each element $x^{(i)}$ in the input sequence</b> by incorporating information from all other elements in the sequence</span> (figure 3.7).\n",
    "  - <span style=\"color:#4ea9fb\">Context vectors play a crucial role in self-attention</span>.\n",
    "  - <span style=\"color:#4ea9fb\">A context vector</span> $z^{(i)}$ <span style=\"color:#4ea9fb\"> is a weighted sum of all inputs </span> , $x^{(1)}$ to $x^{(T)}$</span>.\n",
    "  - For e.g., context vector $z^{(2)}$ is an embedding that contains information about $x^{(2)}$ and all other input elements, $x^{(1)}$ to $x^{(T)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">‚ö†Ô∏è Below formulas and table clearly explains how different components are computed.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:#4ea9fb\">Query</span>, $x^{(2)}$\n",
    "- <span style=\"color:#4ea9fb\">Attention score</span>, $\\omega^{(ij)} = x^{(i)} \\cdot  x^{(j)}$\n",
    "  - $\\implies \\omega^{(21)} = x^{(2)} \\cdot  x^{(1)}$\n",
    "  -  $\\implies \\omega^{(2)} = \\left[ \\omega^{(21)} \\quad \\omega^{(22)} \\quad \\omega^{(23)} \\quad \\omega^{(26)} \\right] = \\left[ x^{(2)} \\cdot  x^{(1)} \\quad  x^{(2)} \\cdot  x^{(2)} \\quad  x^{(2)} \\cdot  x^{(3)} \\quad  \\dots \\quad  x^{(2)} \\cdot  x^{(6)} \\right]$    \n",
    "- <span style=\"color:#4ea9fb\">Attention weights</span>, $\\alpha^{(i)} = \\text{softmax}(\\omega^{(i)})$\n",
    "  -  $\\implies \\alpha^{(2)} = \\left[ \\alpha^{(21)} \\quad \\alpha^{(22)} \\quad \\alpha^{(23)} \\quad \\dots \\quad \\alpha^{(26)} \\right] = \\text{softmax}(\\omega^{(2)}) $\n",
    "-  <span style=\"color:#4ea9fb\">Context vector</span>, $z^{(i)} = \\sum_{j=1}^{T} \\alpha^{(ij)} x^{(j)}$\n",
    "   -  $\\implies z^{(2)} = \\sum_{j=1}^{T} \\alpha^{(2j)} x^{(j)} = \\alpha^{(21)} x^{(1)} + \\alpha^{(22)} x^{(2)} + \\alpha^{(23)} x^{(3)} + \\dots + \\alpha^{(26)} x^{(6)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Notation | Shape | Dimensions \n",
    "| :-----------: |:------------: | ------------|\n",
    "| $x$        | 6 x 3       |    2D tensor with 6 rows and 3 columns |\n",
    "| $x^{(2)}$        | 3       | 1D tensor with 3 elements |\n",
    "| $\\omega^{(2)}$        | 6       | 1D tensor with 6 elements |\n",
    "| $\\alpha^{(2)}$        | 6       | 1D tensor with 6 elements |\n",
    "| $\\alpha^{(2j)}$        | 1       | 1D tensor with 1 element |\n",
    "| $z^{(2)}$        | 3       | 1D tensor with 3 elements |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/figure-3.7-goal-of-self-attention-compute-context-vector.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">üìù Let's implement a simplified self-attention mechanism to compute the weights and resulting context vector, step-by-step.</p>\n",
    "\n",
    "In the below example, \n",
    "- `context_length = 6`\n",
    "- `embed_size = 3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "### Input token embeddings ###\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],  # Your    (x^1)\n",
    "        [0.55, 0.87, 0.66],  # journey (x^2)\n",
    "        [0.57, 0.85, 0.64],  # starts  (x^3)\n",
    "        [0.22, 0.58, 0.33],  # with    (x^4)\n",
    "        [0.77, 0.25, 0.10],  # one     (x^5)\n",
    "        [0.05, 0.80, 0.55],  # step    (x^6)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/figure-3.8-computation-of-context-vector-illustration.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: tensor([0.5500, 0.8700, 0.6600])\n",
      "\n",
      "input idx | input_element.input_query:\t\t\t\t\t\t| attn_scores_2\n",
      " 0        | tensor([0.4300, 0.1500, 0.8900]).tensor([0.5500, 0.8700, 0.6600])   | 0.954\n",
      " 1        | tensor([0.5500, 0.8700, 0.6600]).tensor([0.5500, 0.8700, 0.6600])   | 1.495\n",
      " 2        | tensor([0.5700, 0.8500, 0.6400]).tensor([0.5500, 0.8700, 0.6600])   | 1.475\n",
      " 3        | tensor([0.2200, 0.5800, 0.3300]).tensor([0.5500, 0.8700, 0.6600])   | 0.843\n",
      " 4        | tensor([0.7700, 0.2500, 0.1000]).tensor([0.5500, 0.8700, 0.6600])   | 0.707\n",
      " 5        | tensor([0.0500, 0.8000, 0.5500]).tensor([0.5500, 0.8700, 0.6600])   | 1.087\n",
      "\n",
      "attn_scores_2 = tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "### Input embeddings to Attention scores ###\n",
    "query = inputs[1]  # \"journey\" (x^2)    second input token embedding\n",
    "print(f\"query: {query}\")\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "print(f\"\\ninput idx | input_element.input_query:\\t\\t\\t\\t\\t\\t| attn_scores_2\")\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "    print(f\"{i:2d}        | {x_i}.{query}   | {attn_scores_2[i]:.3f}\")\n",
    "print(f\"\\nattn_scores_2 = {attn_scores_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">üí° <b>Understanding dot products</b><br>\n",
    "- It's a concise way of multiplying two vectors element-wise and then summing their products (i.e., sum of element-wise multiplication)<br><br>\n",
    "- It's also a measure of similarity between two vectors, where higher dot product implies higher similarity, and vice versa.<br>\n",
    "- <span style=\"color:green\">In the context of self-attention mechanisms, <b>the dot product determines the extent to which each element in a sequence \"attends to\" (focuses on) any other element</b>; the higher the dot product, the higher the similarity and attention score between two elements, thus more attention is paid to that element.</span>\n",
    "</p>\n",
    "\n",
    "<b>Why normalization of attention scores, $\\omega^{(i)}$?</b>\n",
    "- To obtain the attention weigths that sum up to 1.\n",
    "- Useful for interpretation and maintaining training stability in an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores to Attention weights (Rudimentary approach)\n",
      "attn_weights_2 = tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: 1.000\n",
      "\n",
      "Attention scores to Attention weights (Softmax)\n",
      "attn_weights_2 = tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: 1.000\n",
      "\n",
      "Attention scores to Attention weights (Softmax) - PyTorch\n",
      "attn_weights_2 = tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: 1.000\n"
     ]
    }
   ],
   "source": [
    "### Attention scores to Attention weights ###\n",
    "print(f\"Attention scores to Attention weights (Rudimentary approach)\")\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(f\"attn_weights_2 = {attn_weights_2_tmp}\")\n",
    "print(f\"Sum: {attn_weights_2_tmp.sum():.3f}\")\n",
    "\n",
    "\n",
    "print(f\"\\nAttention scores to Attention weights (Softmax)\")\n",
    "\n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(f\"attn_weights_2 = {attn_weights_2_naive}\")\n",
    "print(f\"Sum: {attn_weights_2_naive.sum():.3f}\")\n",
    "\n",
    "print(f\"\\nAttention scores to Attention weights (Softmax) - PyTorch\")\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(f\"attn_weights_2 = {attn_weights_2}\")\n",
    "print(f\"Sum: {attn_weights_2.sum():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "context_vec_2 = tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "### Attention weights to Context vector ###\n",
    "query = inputs[1]  # \"journey\" (x^2)    second input token embedding\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2_naive[i] * x_i\n",
    "print(f\"\\ncontext_vec_2 = {context_vec_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:black; background-color:#F5C780; padding:15px\">üí° <b>Use softmax function for normalization</b><br>\n",
    "- Softmax function is better at managing extreme values and offers more favorable gradient properties during training.<br>\n",
    "- <b><span style=\"color:green\">Softmax function ensures that the attention weights are always positive</span>, <span style=\"color:red\">unlike simple normalization</span></b>.<br>\n",
    "  - This <b>makes the output interpretable as probabilities, where higher weights indicate greater importance</b>.<br><br>\n",
    "üí° <b>Why use PyTorch implementation of softmax?</b><br>\n",
    "- <span style=\"color:red\">Naive softmax implementation can lead to numerical instability, especially when dealing with large or small input values</span>.<br>\n",
    "- <span style=\"color:green\">PyTorch implementaion of softmax has been extensively tested and optimized for numerical stability.</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Simple normalization]        attn_weights_temp = tensor([ 0.5000, -0.2500,  0.7500])\n",
      "[Softmax based normalization] attn_weights_temp = tensor([0.3482, 0.0777, 0.5741])\n"
     ]
    }
   ],
   "source": [
    "# Normalization without and with softmax\n",
    "attn_scores_temp = torch.tensor([1.0, -0.5, 1.5])\n",
    "attn_weights_tmp = attn_scores_temp / attn_scores_temp.sum()\n",
    "print(f\"[Simple normalization]        attn_weights_temp = {attn_weights_tmp}\")\n",
    "\n",
    "attn_weights_tmp = attn_scores_temp / attn_scores_temp.sum()\n",
    "attn_weights_tmp = softmax_naive(attn_scores_temp)\n",
    "print(f\"[Softmax based normalization] attn_weights_temp = {attn_weights_tmp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/figure-3.9-obtain-attention-weights-example.webp\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/figure-3.10-compute-context-vector-example.webp\" width=\"700px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/figure-3.11-attention-weights-heatmap-example.webp\" width=\"700px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/figure-3.12-three-step-process-to-compute-context-vectors.webp\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/figure-3.7-goal-of-self-attention-compute-context-vector.webp\" width=\"800px\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3_10_build_llm_form_scratch",
   "language": "python",
   "name": "py_3_10_build_llm_form_scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
